{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMT2uVFejWQTf0uPao6mYI+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manohargadde/wordembeddings/blob/main/word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **One Hot Encoding Example**"
      ],
      "metadata": {
        "id": "ByD4c_aNJa2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the corpus of text\n",
        "corpus = [\n",
        "\t\"The quick brown fox jumped over the lazy dog.\",\n",
        "\t\"She sells seashells by the seashore.\",\n",
        "\t\"Peter Piper picked a peck of pickled peppers.\"\n",
        "]"
      ],
      "metadata": {
        "id": "jd8vnzijJOT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a set of unique words in the corpus\n",
        "unique_words = set()\n",
        "for sentence in corpus:\n",
        "\tfor word in sentence.split():\n",
        "\t\tunique_words.add(word.lower())\n",
        "print(unique_words)"
      ],
      "metadata": {
        "id": "O_WVhHGQQ4oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a dictionary to map each\n",
        "# unique word to an index\n",
        "word_to_index = {}\n",
        "for i, word in enumerate(unique_words):\n",
        "\tword_to_index[word] = i\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "id": "uMoB6w35RExc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create one-hot encoded vectors for\n",
        "# each word in the corpus\n",
        "one_hot_vectors = []\n",
        "for sentence in corpus:\n",
        "\tsentence_vectors = []\n",
        "\tfor word in sentence.split():\n",
        "\t\tvector = np.zeros(len(unique_words))\n",
        "\t\tvector[word_to_index[word.lower()]] = 1\n",
        "\t\tsentence_vectors.append(vector)\n",
        "\tone_hot_vectors.append(sentence_vectors)\n",
        "\n",
        "# Print the one-hot encoded vectors\n",
        "# for the first sentence\n",
        "print(\"One-hot encoded vectors for the first sentence:\")\n",
        "for vector in one_hot_vectors[0]:\n",
        "\tprint(vector)"
      ],
      "metadata": {
        "id": "9Z2Y7IWiRQqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bag of words Example**"
      ],
      "metadata": {
        "id": "FxwhCtxkJoIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "sentences = [\n",
        "\t\"The quick brown fox jumped over the lazy dog.\",\n",
        "\t\"She sells seashells by the seashore.\",\n",
        "\t\"Peter Piper picked a peck of pickled peppers.\"\n",
        "]\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the data\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Convert to DataFrame for better readability\n",
        "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "v3-2gv_kJuRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Term Frequency**"
      ],
      "metadata": {
        "id": "6mw32JXDRiFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Term Frequency without Normalisation is same as above. Example below\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "sentences = [\n",
        "\t\"The quick brown fox jumped over the lazy dog.\",\n",
        "\t\"She sells seashells by the seashore.\",\n",
        "\t\"Peter Piper picked a peck of pickled peppers.\"\n",
        "]\n",
        "\n",
        "# Initialize TfidfVectorizer with no IDF normalization\n",
        "vectorizer = TfidfVectorizer(use_idf=False, norm=None)\n",
        "\n",
        "# Fit and transform the data\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "kkXX_-kTRlPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Term Frequency with Normalisation\n",
        "\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "sentences = [\n",
        "\t\"The quick brown fox jumped over the lazy dog.\",\n",
        "\t\"She sells seashells by the seashore.\",\n",
        "\t\"Peter Piper picked a peck of pickled peppers.\"\n",
        "]\n",
        "\n",
        "# Tokenize and compute term frequency\n",
        "def compute_term_frequency(sentences):\n",
        "    tf_data = []\n",
        "    for sentence in sentences:\n",
        "        words = sentence.lower().split()\n",
        "        word_count = Counter(words)\n",
        "        total_words = len(words)\n",
        "        tf = {word: count / total_words for word, count in word_count.items()}\n",
        "        tf_data.append(tf)\n",
        "    return tf_data\n",
        "\n",
        "# Compute Term Frequency\n",
        "tf_data = compute_term_frequency(sentences)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(tf_data).fillna(0)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "hUOwdv-xS6b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TF-IDF**"
      ],
      "metadata": {
        "id": "xCGt8LIRTT92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "sentences = [\n",
        "\t\"The quick brown fox jumped over the lazy dog.\",\n",
        "\t\"She sells seashells by the seashore.\",\n",
        "\t\"Peter Piper picked a peck of pickled peppers.\"\n",
        "]\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the data\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Convert to DataFrame for better readability\n",
        "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "NTyV5XuCTWpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word2Vec**"
      ],
      "metadata": {
        "id": "8sHDw_CjWiTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Sample data\n",
        "sentences = [\n",
        "\t\"The quick brown fox jumped over the lazy dog.\",\n",
        "\t\"She sells seashells by the seashore.\",\n",
        "\t\"Peter Piper picked a peck of pickled peppers.\"\n",
        "]\n",
        "\n",
        "# Preprocess sentences: tokenization and lowercasing\n",
        "processed_sentences = [simple_preprocess(sentence) for sentence in sentences]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=processed_sentences, vector_size=50, window=3, min_count=1, sg=0)\n",
        "#vector_size=50: Number of dimensions for the word vectors.\n",
        "#window=3: Maximum distance between the current and predicted word within a sentence.\n",
        "#min_count=1: Ignores all words with a total frequency lower than this.\n",
        "#sg=0: Uses Continuous Bag of Words (CBOW) model. Use sg=1 for Skip-gram model.\n",
        "\n",
        "# Retrieve word vectors\n",
        "words = list(model.wv.index_to_key)\n",
        "word_vectors = {word: model.wv[word] for word in words}\n",
        "#model.wv.index_to_key: List of words in the model's vocabulary.\n",
        "#model.wv[word]: Vector for a specific word.\n",
        "\n",
        "# Print word vectors\n",
        "for word, vector in word_vectors.items():\n",
        "    print(f\"Word: {word}\")\n",
        "    print(f\"Vector: {vector}\\n\")\n"
      ],
      "metadata": {
        "id": "9iqqB-lGU5oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FastText**"
      ],
      "metadata": {
        "id": "JUe5GgRNWk3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Sample data\n",
        "sentences = [\n",
        "\t\"The quick brown fox jumped over the lazy dog.\",\n",
        "\t\"She sells seashells by the seashore.\",\n",
        "\t\"Peter Piper picked a peck of pickled peppers.\"\n",
        "]\n",
        "\n",
        "# Preprocess sentences: tokenization and lowercasing\n",
        "processed_sentences = [simple_preprocess(sentence) for sentence in sentences]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = FastText(sentences=processed_sentences, vector_size=50, window=3, min_count=1, sg=0)\n",
        "#vector_size=50: Number of dimensions for the word vectors.\n",
        "#window=3: Maximum distance between the current and predicted word within a sentence.\n",
        "#min_count=1: Ignores all words with a total frequency lower than this.\n",
        "#sg=0: Uses Continuous Bag of Words (CBOW) model. Use sg=1 for Skip-gram model.\n",
        "\n",
        "# Retrieve word vectors\n",
        "words = list(model.wv.index_to_key)\n",
        "word_vectors = {word: model.wv[word] for word in words}\n",
        "#model.wv.index_to_key: List of words in the model's vocabulary.\n",
        "#model.wv[word]: Vector for a specific word.\n",
        "\n",
        "# Print word vectors\n",
        "for word, vector in word_vectors.items():\n",
        "    print(f\"Word: {word}\")\n",
        "    print(f\"Vector: {vector}\\n\")\n"
      ],
      "metadata": {
        "id": "Tx1FPwRaWm_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GloVe**"
      ],
      "metadata": {
        "id": "2MKaAgpiXnmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained GloVe vectors\n",
        "def load_glove_vectors(glove_file):\n",
        "    # Create a dictionary to hold word vectors\n",
        "    word_vectors = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            word_vectors[word] = vector\n",
        "    return word_vectors\n",
        "\n",
        "# Example usage\n",
        "#You can download pre-trained GloVe vectors from GloVe's official website. https://nlp.stanford.edu/projects/glove/\n",
        "#For example, glove.6B.zip contains vectors of various dimensions (50d, 100d, 200d, 300d).\n",
        "glove_file = 'glove.6B.50d.txt'  # Replace with the path to your GloVe file\n",
        "word_vectors = load_glove_vectors(glove_file)\n",
        "\n",
        "# Print vector for a specific word\n",
        "word = 'cat'\n",
        "if word in word_vectors:\n",
        "    print(f\"Vector for '{word}':\\n{word_vectors[word]}\")\n",
        "else:\n",
        "    print(f\"Word '{word}' not found in GloVe vectors.\")\n"
      ],
      "metadata": {
        "id": "yps_LY33XpMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BERT**"
      ],
      "metadata": {
        "id": "0bTfOD9SdcEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# Loads the BERT tokenizer. The 'bert-base-uncased' model is a smaller variant of BERT trained on uncased English text.\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "#Loads the pre-trained BERT model.\n",
        "\n",
        "# Example text\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize input text\n",
        "#Tokenizes the input text and converts it into PyTorch tensors. This is needed for feeding the input into the model.\n",
        "inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "# Forward pass through BERT model\n",
        "with torch.no_grad():\n",
        "    #Passes the tokenized input through the BERT model to obtain embeddings.\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Get the embeddings for the first token in the sentence\n",
        "# outputs['last_hidden_state'] contains embeddings for all tokens in the input sequence. The shape of this tensor is (batch_size, sequence_length, hidden_size).\n",
        "last_hidden_state = outputs.last_hidden_state\n",
        "print(f\"Shape of last_hidden_state: {last_hidden_state.shape}\")\n",
        "\n",
        "# Print embeddings for the first token ([CLS] token). Extracts the embedding for the [CLS] token, which is often used for classification tasks.\n",
        "cls_embedding = last_hidden_state[0][0]\n",
        "print(f\"Embedding for [CLS] token: {cls_embedding}\")\n",
        "\n",
        "# Example: Print embeddings for each token\n",
        "# convert_ids_to_tokens - Converts token IDs back to token strings for better readability.\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "for token, embedding in zip(tokens, last_hidden_state[0]):\n",
        "    print(f\"Token: {token}, Embedding: {embedding}\")\n"
      ],
      "metadata": {
        "id": "4_IYENr4decR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ELMo**"
      ],
      "metadata": {
        "id": "uu1UNZQXgG3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained ELMo model from TensorFlow Hub\n",
        "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
        "\n",
        "# Example sentences\n",
        "sentences = [\"The quick brown fox jumps over the lazy dog\",\n",
        "             \"The dog barked at the cat\"]\n",
        "\n",
        "# Define a function to get ELMo embeddings\n",
        "def get_elmo_embeddings(sentences):\n",
        "    #elmo.signatures- Retrieves the embeddings for the input sentences\n",
        "    embeddings = elmo.signatures['default'](tf.constant(sentences))\n",
        "    # embeddings['elmo'] - Contains the embeddings for the sentences.\n",
        "    return embeddings['elmo']\n",
        "\n",
        "# Get ELMo embeddings\n",
        "embeddings = get_elmo_embeddings(sentences)\n",
        "\n",
        "# Convert embeddings to NumPy arrays\n",
        "elmo_embeddings = [np.array(embedding) for embedding in embeddings.numpy()]\n",
        "\n",
        "# Print embeddings for the first sentence\n",
        "print(f\"ELMo embeddings for the first sentence:\")\n",
        "print(elmo_embeddings[0])\n"
      ],
      "metadata": {
        "id": "j8JGA8HjgIq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSE**"
      ],
      "metadata": {
        "id": "39_Vq-o2jbOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"The dog barked at the cat\",\n",
        "    \"The cat and the dog played\",\n",
        "    \"The quick brown fox\"\n",
        "]\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Apply LSA (TruncatedSVD) to the TF-IDF matrix\n",
        "# Performs dimensionality reduction using SVD. The n_components parameter specifies the number of dimensions to reduce to (e.g., topics or latent dimensions)\n",
        "lsa = TruncatedSVD(n_components=2)  # Number of topics or components\n",
        "X_lsa = lsa.fit_transform(X)\n",
        "\n",
        "# Print the shape of the LSA matrix\n",
        "print(f\"Shape of LSA matrix: {X_lsa.shape}\")\n",
        "\n",
        "# Display the transformed documents\n",
        "for i, doc in enumerate(X_lsa):\n",
        "    print(f\"Document {i}: {doc}\")"
      ],
      "metadata": {
        "id": "aLZbjJOvjdGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"The dog barked at the cat\",\n",
        "    \"The cat and the dog played\",\n",
        "    \"The quick brown fox\"\n",
        "]\n",
        "\n",
        "# Create a CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Apply LDA\n",
        "# Performs topic modeling. The n_components parameter specifies the number of topics.\n",
        "lda = LatentDirichletAllocation(n_components=2, random_state=0)\n",
        "X_lda = lda.fit_transform(X)\n",
        "\n",
        "# Print the shape of the LDA matrix\n",
        "print(f\"Shape of LDA matrix: {X_lda.shape}\")\n",
        "\n",
        "# Display the topic distribution for each document\n",
        "for i, doc in enumerate(X_lda):\n",
        "    print(f\"Document {i}: {doc}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BBCV5Ht3kL88"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}